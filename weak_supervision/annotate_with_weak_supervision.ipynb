{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4191e032-1fdd-4aea-93b4-0a0e25b5e129",
   "metadata": {},
   "outputs": [],
   "source": [
    "import skweak\n",
    "import gensim\n",
    "import spacy\n",
    "import re\n",
    "import pandas as pd\n",
    "import pickle as pk\n",
    "import itertools\n",
    "import os\n",
    "\n",
    "from skweak.base import CombinedAnnotator\n",
    "from spacy.training import iob_to_biluo\n",
    "from typing import List, Set\n",
    "from tqdm.auto import tqdm\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from typing import List\n",
    "\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32324500-2dd6-411a-8976-05f1ede512b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('nl_core_news_lg', disable = ['lemmatizer','ner','attribute_ruler','tagger'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b688886-1d60-4c9c-b5c7-85c5cd333823",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = \"dataset/only_known_products_stratified_sample.csv.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e8fa2f4-2dee-47de-b158-a992b1175a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "useful_columns = ['my_problem_is']\n",
    "os.getcwd()\n",
    "df = pd.read_csv(test_dataset, delimiter=\",\",usecols=useful_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66d685bd-ca3c-4505-b4f3-6788e5d5f319",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIEPROFUN = [\"pakket\",\"hussel\",\"kpn een mkb\", \"kpn 1 mkb\", \"kpn_een\", \"kpn een\", \n",
    "               \"contract\",'bundel','dienst','databundel',\"data\", \"sim only\",\"mijnkpn\"\n",
    "               ,'cloud','dsl',\"wifi\",\"glasvezel\", \"smartphone\",\"mobiel\",\"mobiele\",\n",
    "               \"mobieltje\", \"uploadsnelheid\",\"downloadsnelheid\",\"snelheid\", \"gigabyte\", \n",
    "               'iphone','samsung','laptop', \"viaplay\", \"netflix\", \"spotify\", \"eherkenning\"\n",
    "               , \"simkaart\", \"simkaartje\",\"simkaartnummer\",\"pukcode\",\"pin\", \"pinautomaat\",\n",
    "               \"pinnummer\",\"puk\",\"site\",'portal','mijnkpn','kpn',\n",
    "              ]\n",
    "NETWORK = [\"internet\",\"4g\",\"5g\",\"draad\",  \"signaal\", \"punt\", \"lijn\",\"kanaal\",\"kabel\", \"draad\", \"dns\", \"bereik\", \"bandbreedte\", \"ader\"]\n",
    "# ADDRESS = ['adres']\n",
    "HARDWARE = [\"modem\",\"router\",\"fritzbox\",\"fritz\",\"experia\",\"experiabox\",\"tv\", \"televisie\", \"television\", \"pc\",\"computer\"\n",
    "            \"versterker\",\"toestel\",\"camera\",\"card\",\"switch\",\"lamp\",\"kast\", \"zekeringskast\",\"pas\", \"meterkast\", \"desktop\", \"laptop\" , \"camera\",\n",
    "           \"apparaat\", ]\n",
    "\n",
    "TIJD = ['dag','week','uur','maand','maandag','tijd',\"vandaag\",\"nacht\"]\n",
    "COMPETITOR = ['ziggo','vodafone']\n",
    "PROBLEM = [\"fout\",\"fouten\",\"error\",\"probleem\",\"problemen\",\"storing\",\"incident\", \"schade\", \"klacht\", \"conflict\"\n",
    "                  \"firewall\",\"virus\",\"virussen\",'fraude','oplichting','melding']\n",
    "PERSON = [\"ik\", \"we\",\"Ik\",\"We\"]\n",
    "BILLING = ['factuur','facturen','incassobureau','termijnen','afbetaling'\n",
    "            ,\"euro\",'kosten','geld','betaling',\"korting\", 'betalingsachterstand', \"bankrekening\"]\n",
    "\n",
    "COMMUNICATION = ['sms','mail','voicemail']\n",
    "CONTRACT = ['abonnement', 'contract','abonnementje']\n",
    "LOCATION = ['locatie','huis','kantoor','winkel','stationlocatie','home', \"etage\", \"adress\", \"wijk\", \"buurt\"]\n",
    "\n",
    "MECHANIC = ['monteur']\n",
    "PROCESS = ['work', 'vertraging', 'verplaatsing','verhuizing','transitie','renovatie','proces','implementatie','actie']\n",
    "CONNECTION = ['verbinding', 'aansluiting','koppeling']\n",
    "NUMBER = ['nummer','telefoonnummer']\n",
    "ORDER = ['order', 'opdracht', 'bestelling']\n",
    "OFFER = [\"aanvraag\",\"aanbieding\", \"offerte\"]\n",
    "PLANNING = [\"week\", \"bevestiging\", \"afspraak\"]\n",
    "SEGMENT = [\"segment\", \"kleinzakelijk\", \"grootzakelijk\"]\n",
    "OTHER = ['cadeau', \"verpakking\", \"functie\", \"test\", \"oorzaak\",'status','blokkering']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45da7418-a9b1-4a54-b92d-3fee5ee917f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_ner_classes = [DIEPROFUN,NETWORK,HARDWARE,TIJD,COMPETITOR,PROBLEM,PERSON,BILLING,COMMUNICATION,CONTRACT,LOCATION, MECHANIC, PROCESS,\n",
    "                        CONNECTION, NUMBER, ORDER, OFFER, PLANNING, SEGMENT, OTHER]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f6a8b90-5e40-4e22-bd45-cfb1bc6c0f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_model = gensim.models.KeyedVectors.load(\"MODEL_GOES_HERE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29a76c54-c62d-4f25-ab44-4110e388bb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_candidate_synonyms(ner_class: List[str]):\n",
    "    candidates = []\n",
    "    print(ner_class)\n",
    "    for word in ner_class:\n",
    "        try:\n",
    "            top10_most_similar = similarity_model.wv.most_similar(word, topn=30)\n",
    "            top10_most_similar = [word[0].replace(\"_\",\" \") for word in top10_most_similar if word[1] >= 0.80 and not (word[0] in ['haha','heb','oke'])]\n",
    "            top10_most_similar = [word for word in top10_most_similar if len(word.split(\" \")) < 2]\n",
    "            candidates.append(top10_most_similar)\n",
    "        except:\n",
    "            print(\"{} is OOV\".format(word))\n",
    "            continue\n",
    "    return candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96a6d9f-4450-40f2-8fa8-14cfbe537f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_labels = []\n",
    "for ner_class in original_ner_classes:\n",
    "    candidates = add_candidate_synonyms(ner_class)\n",
    "    candidates = candidates\n",
    "    merged = list(itertools.chain(*candidates)) + ner_class\n",
    "\n",
    "    updated_labels.append(list(set(merged)))\n",
    "\n",
    "assert len(original_ner_classes) == len(updated_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a391879-5584-405c-8f43-9d39e4cf937b",
   "metadata": {},
   "outputs": [],
   "source": [
    "transp = pd.DataFrame(data=updated_labels,index=None).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b637f820-288f-4355-89e9-8e336df17965",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m transp\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m \u001b[43mlabels\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'labels' is not defined"
     ]
    }
   ],
   "source": [
    "transp.columns = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "d9068c7b-933f-4b7b-b900-425c216feb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "transp.to_csv(\"expanded_ner_classes.csv\", encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c5df2b3-44c6-497a-a885-56f71e87cd94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ['simkaarten', 'viaplay', 'pin', 'gigabyte', 'ongelimiteerd', 'tumtum', 'giechelen', 'glasvezels', 'laptop', 'spotify', 'uitzendt', 'uitgediend', 'tewerkstellingsvergunning', 'pinautomaat', 'computer', 'erkenning', 'kassasysteem', 'wifiverbinding', 'mobiele', 'hussel', 'snelheid', 'lokmiddel', 'bundel', 'ongelimiteerde', 'android', 'tablet', 'gigabytes', 'simcard', 'alarmsysteem', 'internetsnelheden', 'kpn 1 mkb', 'portal', 'porto', 'mbits', 'onbeperkt', 'internetsnelheid', 'pakketten', 'pincode', 'glasvezelaansluiting', 'contract', 'mobieltje', 'bundeltje', 'iphone', 'site', 'snelheden', 'mijnkpn', 'russell', 'smartphone', 'pinnummer', 'unlimited', 'uploadsnelheid', 'dsl', 'gb', 'glasvezelverbinding', 'website', 'gig', 'data', 'netflix', 'databundel', 'giechel', 'simkaartje', 'internetbundel', 'internetpakket', 'simkaart', 'glasvezel', 'sim', 'aanleggen', 'pukcode', 'koper', 'kpn een mkb', 'glasvezelkabel', 'contracten', 'dienst', 'kpn_een', 'portaal', 'kpn een', 'voordeelbundel', 'leemte', 'aangelegd', 'giechelt', 'glas', 'bundels', 'galaxy', 'pleegde', 'pakket', 'contracttermijn', 'wifi', 'kpn', 'puk', 'eherkenning', 'downloadsnelheid', 'mobiel', 'a50', 'pinautomaten', 'belbundel', 'mbit', 'binnengebracht', 'sim only', 'pinapparaat', 'simkaartnummer', 'videoland', 'vaste', 'samsung', 'ipad', 'netflixen', 'cloud', 'grief', 'belminuten', 'booster', 'kaartje']\n",
      "1 ['kabels', 'internetten', 'kabel', 'ethernetkabel', 'bekabeld', 'bandbreedte', 'internet', 'jep', '4g', 'draadje', 'draad', 'getwist', 'bekabelde', 'netwerkkabel', '5g', 'internetverbinding', 'ingelaste', 'telefoonkabel', 'draadloos', 'an', 'utp', 'internetkabel', 'bereik', 'ingeprikt', 'dns', 'bedrading', 'ader', 'draadjes', 'kanaal', 'televisie', 'punt', 'lijn', 'signaal', 'kabeltje']\n",
      "2 ['pas', 'fritzbox', 'aansluitpunt', 'muur', 'router', 'internet', 'card', 'xperia', 'voordeur', 'kelder', 'switches', 'power', 'computerversterker', 'switchen', 'citrix', 'experiabox', 'meterkast', 'telefoontoestel', 'fritz', 'lichtje', 'switch', 'modem', 'desktop', 'apparaat', 'zekeringskast', 'interactief', 'lamp', 'televisies', 'glasvezelmodem', 'hoofdaansluiting', 'laptop', 'glasvezelkastje', 'bodem', 'gemonteerd', 'experia', 'verdeel', 'camera', 'television', 'computer', 'toestel', 'rouwt', 'routers', 'apparaten', 'cisco', 'petje', 'doorgetrokken', 'afgemonteerd', 'kast', 'televisie', 'expedia', 'tv', 'pc']\n",
      "3 ['donderdag', 'week', 'kwart', 'nacht', 'vandaag', 'dinsdag', 'uur', 'weekje', 'vrijdag', 'maandagmiddag', 'halfnegen', 'zaterdag', 'maandag', 'tijd', 'halfelf', 'weken', 'vanmiddag', 'half', 'maandagochtend', 'maand', 'dag', 'halftwee', 'uiterlijk', 'periode', 'woensdagavond', 'woensdag']\n",
      "4 ['vodaphone', 'ziggo', 'vodafone', 'hollandsnieuwe']\n",
      "5 ['fraude', 'negenen', 'zuiverder', '365', 'vuisten', 'incest', 'nuttig', 'verstoring', 'vijver', 'ticketnummer', 'ticket', 'virus', 'prachtig', 'zeef', 'zuster', 'vestig', 'dringen', 'conflictfirewall', 'aanmelding', 'achten', 'zussen', 'sussen', 'schrijfster', 'achterneef', 'schade', 'zeventigers', 'fouten', 'error', 'zesde', 'internetstoring', 'klacht', 'negentigste', 'melding', 'klachten', 'zevende', 'oplichting', 'reces', 'klachtenafdeling', 'probleem', 'zeventigste', 's7', 'incidentje', 'fout', 'storing', 'negens', 'prinses', 'd66', 'zesen', 'foutmelding', 'negende', 'virussen', 'storingen', 'zessen', 'incident', 'problemen', 'zevenvoudig', 'vieren', 'stevig', 'sussend', 'innoveert', 's9']\n",
      "6 ['Ik', 'We', 'zouden', 'ik', 'we', 'gaan']\n",
      "7 ['facturen', 'filosofeert', 'korting', 'veertiger', 'banknummer', 'aflossing', 'rekeningnummer', 'abonnementskosten', 'toestelkosten', 'betaalachterstand', 'zestiger', 'priverekening', 'afgelost', 'bankrek', 'toestelkrediet', 'totaalbedrag', 'gespreide', 'cent', 'eurocent', 'factuur', 'geuren', 'euro', 'creditfactuur', 'dertiger', 'iban', 'betalingsregeling', 'rekeningnummers', 'toestelprijs', 'maandbedrag', 'rekeningen', 'aflost', 'twintiger', 'betalingen', 'kortingsactie', 'negentiger', 'betalingsachterstand', 'dertigers', 'bankrekeningen', 'betaalt', 'aflossingen', 'krediet', 'afbetaling', 'vijftiger', 'meerkosten', 'kosten', 'maandfactuur', 'tientje', 'uitbetaling', 'gespreid', 'deurwaarder', 'geld', 'tachtiger', 'termijnen', 'afbetaald', 'kortingen', 'bankrekeningnummer', 'incassobureau', 'bankrekening', 'deftige', 'veertigurige', 'betaling', 'betaalrekening', 'factuurbedrag', 'actiekorting', 'brutoprijs', 'zeventiger', 'afbetalen', 'gerekend']\n",
      "8 ['smash', 'antwoordapparaat', 'sms', 'begroeting', 'mailtje', 'email', 'mail', 'emailtje', 'voicemail', 'mailt', 'smst', 'semester', 'welkomstboodschap']\n",
      "9 ['abbonoment', 'abonnementen', 'uitgediend', 'contracten', 'abonnement', 'internetabonnement', 'contracttermijn', 'telefoonabonnement', 'abonnementsvorm', 'contract', 'internetpakket', 'appartementjes', 'abonnementje', 'amendement']\n",
      "10 ['kantoor', 'home', 'wijk', 'kantoren', 'stationlocatie', 'huis', 'etage', 'verdieping', 'buurt', 'terrein', 'verdiepingen', 'locatie', 'winkel', 'adress', 'vlakbij', 'telefoonwinkel']\n",
      "11 ['servicemonteur', 'monteur', 'storingsmonteur', 'installatiemonteur']\n",
      "12 ['renovatie', 'actie', 'vertraging', 'verhuizingen', 'verplaatsing', 'proces', 'acties', 'verhuizing', 'vertragingen', 'implementatie', 'transitie', 'work']\n",
      "13 ['aansluiting', 'connectie', 'verbinding', 'koppelingen', 'netwerkverbinding', 'internetaansluiting', 'koppeling', 'aansluitingen', 'binding']\n",
      "14 ['telefoonnummer', 'mobielnummer', 'nummer']\n",
      "15 ['opdrachten', 'opdracht', 'order', 'bestelling', 'orders']\n",
      "16 ['aanbod', 'aanbiedingen', 'aanbieding', 'verlengaanbod', 'aanpak', 'offerte', 'kortingsactie', 'aanvraag', 'aanvraagt']\n",
      "17 ['week', 'bevestigd', 'weekje', 'monteursafspraak', 'weken', 'orderbevestiging', 'bevestiging', 'bevestigingsmail', 'afspraak', 'bevestigingen']\n",
      "18 ['kleinzakelijk', 'consumentenmarkt', 'segment', 'grootzakelijk', 'grootzakelijke', 'zakelijk', 'mkb']\n",
      "19 ['blokkering', 'oorzaak', 'status', 'functies', 'blokkeer', 'verpakking', 'blokkade', 'functie', 'test', 'cadeau']\n"
     ]
    }
   ],
   "source": [
    "for index,ner_class in enumerate(updated_labels):\n",
    "    print(index, ner_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7826a537-fa3b-48e9-8879-9156f804a76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DIEPROFUN_entities_detector(doc):\n",
    "\n",
    "    search_range = 2\n",
    "    for index, tok in enumerate(nlp(doc.text)):\n",
    "      if tok.text in updated_labels[0]:\n",
    "        start = tok.i\n",
    "        if index > 0:\n",
    "          distance_from_start = index - 0\n",
    "          # print(distance_from_start)\n",
    "          if distance_from_start < 2:\n",
    "            search_range = distance_from_start\n",
    "\n",
    "          reverse = -1\n",
    "          while ((tok.nbor(reverse).pos_ in ['PROPN', 'NOUN', 'ADJ', 'SYM'] or [\"een\",\"1\"]) and -abs(search_range) < reverse):\n",
    "            if tok.nbor(reverse).pos_ in ['PUNCT', 'VERB', 'DET', 'ADP','PRON']:\n",
    "              break\n",
    "            start = tok.nbor(reverse).i\n",
    "            reverse -= 1\n",
    "        \n",
    "        yield start, index+1, \"DIEPROFUN\"\n",
    "\n",
    "def NETWORK_entities_detector(doc):\n",
    "    for index, tok in enumerate(nlp(doc.text)):\n",
    "        if tok.text.lower() in updated_labels[1]:\n",
    "            yield index, index+1, \"NETWORK\"\n",
    "            \n",
    "\n",
    "def HARDWARE_entities_detector(doc):\n",
    "\n",
    "    search_range = 2\n",
    "    for index, tok in enumerate(nlp(doc.text)):\n",
    "      if tok.text in updated_labels[2]:\n",
    "        start = tok.i\n",
    "        if index > 0:\n",
    "          distance_from_start = index - 0\n",
    "          # print(distance_from_start)\n",
    "          if distance_from_start < 2:\n",
    "            search_range = distance_from_start\n",
    "\n",
    "          reverse = -1\n",
    "          while ((tok.nbor(reverse).pos_ in ['PROPN', 'NOUN', 'ADJ', 'SYM'] or [\"een\",\"1\"]) and -abs(search_range) < reverse):\n",
    "            if tok.nbor(reverse).pos_ in ['PUNCT', 'VERB', 'DET', 'ADP','PRON']:\n",
    "              break\n",
    "            start = tok.nbor(reverse).i\n",
    "            reverse -= 1\n",
    "        \n",
    "        yield start, index+1, \"HARDWARE\"\n",
    "        \n",
    "\n",
    "def TIME_entities_detector(doc):\n",
    "    search_range = 2\n",
    "    for index, tok in enumerate(nlp(doc.text)):\n",
    "        if tok.text in updated_labels[3]:\n",
    "            start = tok.i\n",
    "            if index > 0:\n",
    "              distance_from_start = index - 0\n",
    "              if distance_from_start < 2:\n",
    "                search_range = distance_from_start\n",
    "              reverse = -1\n",
    "              while ((tok.nbor(reverse).pos_ in ['NUM']) and -abs(search_range) < reverse):\n",
    "                  if tok.nbor(reverse).pos_ in ['PUNCT', 'VERB', 'DET', 'ADP', 'POSS', 'PRON']:\n",
    "                    break\n",
    "                  start = tok.nbor(reverse).i\n",
    "                  reverse -= 1\n",
    "\n",
    "            yield start, index+1, \"TIME\"\n",
    "            \n",
    "            \n",
    "def COMPETITOR_entities_detector(doc):\n",
    "     for index, tok in enumerate(nlp(doc.text)):\n",
    "        if tok.text.lower() in updated_labels[4]:\n",
    "            yield index, index+1, \"COMPETITOR\" \n",
    "\n",
    "def PROBLEM_entities_detector(doc):\n",
    "     for index, tok in enumerate(nlp(doc.text)):\n",
    "        if tok.text.lower() in updated_labels[5]:\n",
    "            yield index, index+1, \"PROBLEM\"\n",
    "\n",
    "def PERSON_entities_detector(doc):\n",
    "    for index, tok in enumerate(nlp(doc.text)):\n",
    "        if tok.text.lower() in updated_labels[6]:\n",
    "            yield index, index+1, \"PERSON\" \n",
    "            \n",
    "def BILLING_entities_detector(doc):\n",
    "    for index, tok in enumerate(nlp(doc.text)):\n",
    "        if tok.text.lower() in updated_labels[7]:\n",
    "            yield index, index+1, \"BILLING\"\n",
    "            \n",
    "def COMMUNICATION_entities_detector(doc):\n",
    "    for index, tok in enumerate(nlp(doc.text)):\n",
    "        if tok.text.lower() in updated_labels[8]:\n",
    "            yield index, index+1, \"COMMUNICATION\" \n",
    "\n",
    "def CONTRACT_entities_detector(doc):\n",
    "    for index, tok in enumerate(nlp(doc.text)):\n",
    "        if tok.text.lower() in updated_labels[9]:\n",
    "            yield index, index+1, \"CONTRACT\"\n",
    "\n",
    "def LOCATION_entities_detector(doc):\n",
    "    for index, tok in enumerate(nlp(doc.text)):\n",
    "        if tok.text.lower() in updated_labels[10]:\n",
    "            yield index, index+1, \"LOCATION\"\n",
    "            \n",
    "def MECHANIC_entities_detector(doc):\n",
    "    for index, tok in enumerate(nlp(doc.text)):\n",
    "        if tok.text.lower() in updated_labels[11]:\n",
    "            yield index, index+1, \"MECHANIC\"\n",
    "            \n",
    "def PROCESS_entities_detector(doc):\n",
    "    for index, tok in enumerate(nlp(doc.text)):\n",
    "        if tok.text.lower() in updated_labels[12]:\n",
    "            yield index, index+1, \"PROCESS\"\n",
    "            \n",
    "def CONNECTION_entities_detector(doc):\n",
    "    for index, tok in enumerate(nlp(doc.text)):\n",
    "        if tok.text.lower() in updated_labels[13]:\n",
    "            yield index, index+1, \"CONNECTION\"\n",
    "            \n",
    "def NUMBER_entities_detector(doc):\n",
    "    for index, tok in enumerate(nlp(doc.text)):\n",
    "        if tok.text.lower() in updated_labels[14]:\n",
    "            yield index, index+1, \"NUMBER\"\n",
    "\n",
    "def ORDER_entities_detector(doc):\n",
    "    for index, tok in enumerate(nlp(doc.text)):\n",
    "        if tok.text.lower() in updated_labels[15]:\n",
    "            yield index, index+1, \"ORDER\"\n",
    "            \n",
    "def OFFER_entities_detector(doc):\n",
    "    for index, tok in enumerate(nlp(doc.text)):\n",
    "        if tok.text.lower() in updated_labels[16]:\n",
    "            yield index, index+1, \"OFFER\"\n",
    "            \n",
    "def PLANNING_entities_detector(doc):\n",
    "    for index, tok in enumerate(nlp(doc.text)):\n",
    "        if tok.text.lower() in updated_labels[17]:\n",
    "            yield index, index+1, \"PLANNING\"\n",
    "            \n",
    "def SEGMENT_entities_detector(doc):\n",
    "    for index, tok in enumerate(nlp(doc.text)):\n",
    "        if tok.text.lower() in updated_labels[18]:\n",
    "            yield index, index+1, \"SEGMENT\"\n",
    "\n",
    "def OTHER_entities_detector(doc):\n",
    "    for index, tok in enumerate(nlp(doc.text)):\n",
    "        if tok.text.lower() in updated_labels[19]:\n",
    "            yield index, index+1, \"OTHER\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af938baf-45e7-490d-830d-fc1e5e7eff23",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIEPROFUN_detector = skweak.heuristics.FunctionAnnotator(\"DIEPROFUN\", DIEPROFUN_entities_detector)\n",
    "NETWORK_detector = skweak.heuristics.FunctionAnnotator(\"NETWORK\", NETWORK_entities_detector)\n",
    "HARDWARE_detector = skweak.heuristics.FunctionAnnotator(\"HARDWARE\", HARDWARE_entities_detector)\n",
    "TIME_detector = skweak.heuristics.FunctionAnnotator(\"TIME\", TIME_entities_detector)\n",
    "COMPETITOR_detector = skweak.heuristics.FunctionAnnotator(\"COMPETITOR\", COMPETITOR_entities_detector)\n",
    "PROBLEM_detector = skweak.heuristics.FunctionAnnotator(\"PROBLEM\", PROBLEM_entities_detector)\n",
    "PERSON_detector = skweak.heuristics.FunctionAnnotator(\"PERSON\", PERSON_entities_detector)\n",
    "BILLING_detector = skweak.heuristics.FunctionAnnotator(\"BILLING\", BILLING_entities_detector)\n",
    "COMMUNICATION_detector = skweak.heuristics.FunctionAnnotator(\"COMMUNICATION\", COMMUNICATION_entities_detector)\n",
    "CONTRACT_detector = skweak.heuristics.FunctionAnnotator(\"CONTRACT\", CONTRACT_entities_detector)\n",
    "LOCATION_detector = skweak.heuristics.FunctionAnnotator(\"LOCATION\", LOCATION_entities_detector)\n",
    "MECHANIC_detector = skweak.heuristics.FunctionAnnotator(\"MECHANIC\", MECHANIC_entities_detector)\n",
    "PROCESS_detector = skweak.heuristics.FunctionAnnotator(\"PROCESS\", PROCESS_entities_detector)\n",
    "CONNECTION_detector = skweak.heuristics.FunctionAnnotator(\"CONNECTION\", CONNECTION_entities_detector)\n",
    "NUMBER_detector = skweak.heuristics.FunctionAnnotator(\"NUMBER\", NUMBER_entities_detector)\n",
    "ORDER_detector = skweak.heuristics.FunctionAnnotator(\"ORDER\", ORDER_entities_detector)\n",
    "OFFER_detector = skweak.heuristics.FunctionAnnotator(\"OFFER\", OFFER_entities_detector)\n",
    "PLANNING_detector = skweak.heuristics.FunctionAnnotator(\"PLANNING\", PLANNING_entities_detector)\n",
    "SEGMENT_detector = skweak.heuristics.FunctionAnnotator(\"SEGMENT\", SEGMENT_entities_detector)\n",
    "OTHER_detector = skweak.heuristics.FunctionAnnotator(\"OTHER\", OTHER_entities_detector)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f842b529-f9f6-4fbb-bf25-c8651ef7af5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotators_list = [DIEPROFUN_detector, NETWORK_detector, HARDWARE_detector, TIME_detector, \n",
    "                  COMPETITOR_detector, PROBLEM_detector, PERSON_detector, \n",
    "                   BILLING_detector, COMMUNICATION_detector, CONTRACT_detector, LOCATION_detector,\n",
    "                  MECHANIC_detector, PROCESS_detector,CONNECTION_detector,NUMBER_detector,\n",
    "                  ORDER_detector, OFFER_detector, PLANNING_detector, SEGMENT_detector, OTHER_detector]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "36c1a982-f74b-4f89-bed0-639a35fa5903",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"DIEPROFUN\", \"NETWORK\", \"HARDWARE\",  \"TIME\", \"COMPETITOR\", \"PROBLEM\", \"PERSON\", \"BILLING\", \"COMMUNICATION\",\"CONTRACT\",\"LOCATION\",\n",
    "         \"MECHANIC\", \"PROCESS\", \"CONNECTION\", \"NUMBER\", \"ORDER\", \"OFFER\", \"PLANNING\", \"SEGMENT\", \"OTHER\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "60997349-088e-4b0e-a09c-c5bbd0b1bd5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">en nu is mijn vraag of waar eventueel mijn firewall en tussen virus en cisco ook naar de tv kunnen kijken of zo, want daar hebben geen ziggo tv abonnement voor, ga voor alleen abonnementen als het goed heb begrepen.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******NEXT ANNOTATOR******\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">en nu is mijn vraag of waar eventueel mijn firewall en tussen \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    virus\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PROBLEM</span>\n",
       "</mark>\n",
       " en cisco ook naar de tv kunnen kijken of zo, want daar hebben geen ziggo tv abonnement voor, ga voor alleen abonnementen als het goed heb begrepen.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******NEXT ANNOTATOR******\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">en nu is mijn vraag of waar eventueel mijn firewall en tussen virus \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    en cisco\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">HARDWARE</span>\n",
       "</mark>\n",
       " ook naar de \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    tv\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">HARDWARE</span>\n",
       "</mark>\n",
       " kunnen kijken of zo, want daar hebben geen \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ziggo tv\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">HARDWARE</span>\n",
       "</mark>\n",
       " abonnement voor, ga voor alleen abonnementen als het goed heb begrepen.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******NEXT ANNOTATOR******\n",
      "******NEXT ANNOTATOR******\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">en nu is mijn vraag of waar eventueel mijn firewall en tussen virus en cisco ook naar de tv kunnen kijken of zo, want daar hebben geen ziggo tv abonnement voor, ga voor alleen abonnementen als het goed heb begrepen.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******NEXT ANNOTATOR******\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">en nu is mijn vraag of waar eventueel mijn firewall en tussen virus en cisco ook naar de tv kunnen kijken of zo, want daar hebben geen \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ziggo\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">COMPETITOR</span>\n",
       "</mark>\n",
       " tv abonnement voor, ga voor alleen abonnementen als het goed heb begrepen.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc = nlp(\"en nu is mijn vraag of waar eventueel mijn firewall en tussen virus en cisco ook naar de tv kunnen kijken of zo, want daar hebben geen ziggo tv abonnement voor, ga voor alleen abonnementen als het goed heb begrepen.\")\n",
    "DIEPROFUN_detector(doc)\n",
    "HARDWARE_detector(doc)\n",
    "PROBLEM_detector(doc)\n",
    "PERSON_detector(doc)\n",
    "COMPETITOR_detector(doc)\n",
    "\n",
    "\n",
    "skweak.utils.display_entities(doc, \"DIEPROFUN\")\n",
    "print(\"******NEXT ANNOTATOR******\")\n",
    "skweak.utils.display_entities(doc, \"PROBLEM\")\n",
    "print(\"******NEXT ANNOTATOR******\")\n",
    "skweak.utils.display_entities(doc, \"HARDWARE\")\n",
    "print(\"******NEXT ANNOTATOR******\")\n",
    "skweak.utils.display_entities(doc, \"PERSON\")\n",
    "print(\"******NEXT ANNOTATOR******\")\n",
    "skweak.utils.display_entities(doc, \"COMPETITOR\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1014aa42-832a-467d-9cf8-9da78e9c75a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>my_problem_is</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7336</th>\n",
       "      <td>[customer][nvt][my_problem_is][01:18]: Zo bezi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7337</th>\n",
       "      <td>[customer][nvt][my_problem_is][00:12]: Ik heb ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7338</th>\n",
       "      <td>[customer][nvt][my_problem_is][00:11]: Nee, ni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7339</th>\n",
       "      <td>[customer][nvt][my_problem_is][00:09]: Ja hoor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7340</th>\n",
       "      <td>[customer][unpleasant][my_problem_is][11:09]: ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          my_problem_is\n",
       "7336  [customer][nvt][my_problem_is][01:18]: Zo bezi...\n",
       "7337  [customer][nvt][my_problem_is][00:12]: Ik heb ...\n",
       "7338  [customer][nvt][my_problem_is][00:11]: Nee, ni...\n",
       "7339  [customer][nvt][my_problem_is][00:09]: Ja hoor...\n",
       "7340  [customer][unpleasant][my_problem_is][11:09]: ..."
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df = df\n",
    "sample_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "56fabe3e-a1fc-4154-9952-ca2706a8e9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = sample_df.my_problem_is.values\n",
    "\n",
    "cleaned_texts = [re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", conversation) for conversation in texts]\n",
    "cleaned_texts = [''.join(conversation.lstrip().splitlines()) for conversation in cleaned_texts]\n",
    "cleaned_texts = [''.join(conversation) for conversation in cleaned_texts if len(conversation.split(\" \")) < 550]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "3c9da15c-e94b-4fd5-8f06-016d7d33a133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": Een ja, nou, ik heb een abonnement verlengt en een toestel bij het toestel nou, bijzonder laden. ... alleen, dat is een oud adres en daar kom ik niet veranderen omdat ze zeiden dat ik dat mijnkpn moet doen. : Die staat een paar weken geleden contact gehad dat dat dat werd, stond er en dat is toen aangepast, maar nu wil : Die daar proberen, die afleveren, omruilen ben ik natuurlijk niet. : En ik had gisteren proberen te bellen. : Zit alleen even te kijken, want hier vandaan, vanuit kpn gezien kan ik hem nu niet aanpassen omdat die eigenlijk al ja onderweg is om het zo te zeggen aan ja : Echt met postnl. Ja, ik heb een postnl zo proberen aanpassen dat zo n opnemen. Bij een postnl punt weet ik niet of dat toen omdat het een teken : Nee, alleen aanpassen naar of bij de buren of bij een postnl punt. Maar ik kon niet. : Ja, ja, ik weet niet of dat werkt. Dan ondertussen moest ik het pakket is maar : De in dat er gegevens gewijzigd zijn, dus staat niet specifiek zijn bij welke gegevens. Dat zij natuurlijk er allemaal met privacy te maken. Dus ja, het is puur ter bevestiging van hij. Wij hebben een wijziging doorgekregen. Nou, wij weten dan dat dat dat het gaat om die .... Dat wordt dus verwijderd wordt uit uw.\n",
      ": Ja, ik had een probleem, ja. : Nou, volgens mij dus ik heb een probleem met de inloggen met de kpn. Elke keer heeft vraagt gewoon naar e,mail nummer en als ik mijn e,mail nummer invoeren, herkend. : Oke en gepraat wil voor mijn kpn. : Ik heb ik heb ook een, ik heb ook bundel ontvangen van mijn zoon, dus werkgroep internet en alles en hij heeft voor mij gekeken. 5 deze mb s gestuurd, maar ik krijg die bundels niet. Ik krijg wel een sms je van kpn dat dat ik bundel heb ontvangen en als : Ze was het dit werk moet u me telefoon uitschakelen, weer inschakelen, maar heb ik gedaan en dan krijg ik hem ook niet. Daarna kreeg ik weer een sms je van kpn dat ik, want dat precies wat bundel. : U weer gewoon dat meegestuurd, doet het weer hetzelfde. Dus ik kan helemaal niet inloggen. Bekeken hoeveel mb s ik heb. : Een te berechten geeft die aan van op de e,mail waar ik een rekening en zo kreeg het allemaal voor. Ja. : Ja, want, want dat is dan stuurt elkaar, want ik heb geprobeerd en dat kpn mij een mail sturen, maar die mail komt ook niet. : Ja, ik heb 3 keer mb s gekregen, maar die ontvang ik. : 0800 ..., ja, ja, want gisteren was gedicht tijdens ik heb geprobeerd, maar daarom : Ja, oke, dus daarom maar ik vind het heel raar dat ik die mb s niet van jullie : Ja, nou, internet, niet het ... die het internetbundel moet activeren, dat zat, geven een vertraging in het buitenland op een omni conflict. Ik, wij hebben tot doorgespeeld, ben niet de enige hoor zo meerdere zaken klanten hebben hier ontzettend veel last van, een blijkbaar consumenten klanten nu ook.\n"
     ]
    }
   ],
   "source": [
    "print(cleaned_texts[0])\n",
    "exam = \"Ik heb een duo sim en een telefoon allebei van kpn en nu krijg ik weer geregeld voor werk en ons volgende week ook het geval was\"\n",
    "doc = nlp(exam)\n",
    "print(cleaned_texts[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bd2e982a-3508-48f5-830f-5bbf2c18a960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<skweak.heuristics.FunctionAnnotator object at 0x7fa6f3447a30>\n",
      "<skweak.heuristics.FunctionAnnotator object at 0x7fa6f3447220>\n",
      "<skweak.heuristics.FunctionAnnotator object at 0x7fa6f3447fd0>\n",
      "<skweak.heuristics.FunctionAnnotator object at 0x7fa6f33206a0>\n",
      "<skweak.heuristics.FunctionAnnotator object at 0x7fa6f33204f0>\n",
      "<skweak.heuristics.FunctionAnnotator object at 0x7fa6f34401c0>\n",
      "<skweak.heuristics.FunctionAnnotator object at 0x7fa6f3440280>\n",
      "<skweak.heuristics.FunctionAnnotator object at 0x7fa6f34402b0>\n",
      "<skweak.heuristics.FunctionAnnotator object at 0x7fa6f32cc9d0>\n",
      "<skweak.heuristics.FunctionAnnotator object at 0x7fa6f32cc070>\n",
      "<skweak.heuristics.FunctionAnnotator object at 0x7fa6f32ccbe0>\n",
      "<skweak.heuristics.FunctionAnnotator object at 0x7fa6f333bee0>\n",
      "<skweak.heuristics.FunctionAnnotator object at 0x7fa6f333bbb0>\n",
      "<skweak.heuristics.FunctionAnnotator object at 0x7fa6f3441ac0>\n",
      "<skweak.heuristics.FunctionAnnotator object at 0x7fa6f3441310>\n",
      "<skweak.heuristics.FunctionAnnotator object at 0x7fa6f3441250>\n",
      "<skweak.heuristics.FunctionAnnotator object at 0x7fa6f34412b0>\n",
      "<skweak.heuristics.FunctionAnnotator object at 0x7fa6f3441190>\n",
      "<skweak.heuristics.FunctionAnnotator object at 0x7fa6f3441160>\n",
      "<skweak.heuristics.FunctionAnnotator object at 0x7fa6f3441220>\n"
     ]
    }
   ],
   "source": [
    "rule_based_annotator = CombinedAnnotator()\n",
    "\n",
    "for annotator in annotators_list:\n",
    "    rule_based_annotator.add_annotator(annotator)\n",
    "    print(annotator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "20128841-d0c1-4965-b8ad-f06fb81f0ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before:  6811\n",
      "after :  6811\n"
     ]
    }
   ],
   "source": [
    "print(\"before: \", len(cleaned_texts))\n",
    "new_texts = cleaned_texts\n",
    "print(\"after : \", len(new_texts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ddfe5b71-dd78-41d2-b3d7-546cd93545c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 6811/6811 [29:06<00:00,  3.90it/s]\n"
     ]
    }
   ],
   "source": [
    "new_docs = []\n",
    "conll_docs = \"\"\n",
    "cleaned_texts = new_texts\n",
    "for doc in tqdm(nlp.pipe(cleaned_texts, batch_size=50, n_process=-1), total=len(cleaned_texts)):\n",
    "    doc = rule_based_annotator(doc)\n",
    "    new_docs.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a285be24-8ae7-4b22-834b-1f88f4372747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write to data/annotated_data_3_7...done\n"
     ]
    }
   ],
   "source": [
    "skweak.utils.docbin_writer(new_docs,\"data/annotated_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "27c0b590-6796-45e8-b03d-63e3dcad1fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "hmm = skweak.generative.HMM(\"hmm\", labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "511d2e8e-8998-40a3-8704-bb5d99322a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting iteration 1\n",
      "Number of processed documents: 1000\n",
      "Number of processed documents: 2000\n",
      "Number of processed documents: 3000\n",
      "Number of processed documents: 4000\n",
      "Number of processed documents: 5000\n",
      "Number of processed documents: 6000\n",
      "Finished E-step with 6778 documents\n",
      "Starting iteration 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "         1     -718472.5425             +nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of processed documents: 1000\n",
      "Number of processed documents: 2000\n",
      "Number of processed documents: 3000\n",
      "Number of processed documents: 4000\n",
      "Number of processed documents: 5000\n",
      "Number of processed documents: 6000\n",
      "Finished E-step with 6778 documents\n",
      "Starting iteration 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "         2     -704921.8529      +13550.6896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of processed documents: 1000\n",
      "Number of processed documents: 2000\n",
      "Number of processed documents: 3000\n",
      "Number of processed documents: 4000\n",
      "Number of processed documents: 5000\n",
      "Number of processed documents: 6000\n",
      "Finished E-step with 6778 documents\n",
      "Starting iteration 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "         3     -699784.5878       +5137.2651\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of processed documents: 1000\n",
      "Number of processed documents: 2000\n",
      "Number of processed documents: 3000\n",
      "Number of processed documents: 4000\n",
      "Number of processed documents: 5000\n",
      "Number of processed documents: 6000\n",
      "Finished E-step with 6778 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "         4     -697077.8065       +2706.7814\n"
     ]
    }
   ],
   "source": [
    "hmm.fit(new_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6e01b636-b7a0-40bd-ab9f-19397b590a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = list(hmm.pipe(new_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "69490bf2-0185-496e-b1ca-d02569d17a2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 6811/6811 [00:01<00:00, 6093.34it/s]\n"
     ]
    }
   ],
   "source": [
    "for doc in tqdm(docs):\n",
    "    doc.ents = doc.spans[\"hmm\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "28b299b5-6bed-488b-9309-d1e657d97580",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_extracted = docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "eeba4122-e90e-460e-90ae-db403bc72e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bc2f1097-7e79-49d0-b0e6-a53e70fac049",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = new_extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "003f2053-b95e-4589-a507-365ddfbfb035",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(docs, test_size=0.30, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "945d5733-e67c-4681-bee2-9b66126afffa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4767"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "24f342ca-2ae3-4a84-9617-527577965101",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2044"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "31ff5ea3-fd28-41bf-b8b3-946888ce1117",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Annotation source \"hmm\" cannot be found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [95], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# for doc in docs[214:215]:\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#     skweak.utils.display_entities(doc, \"hmm\") \u001b[39;00m\n\u001b[1;32m      4\u001b[0m hmm\u001b[38;5;241m.\u001b[39mpipe(doc)\n\u001b[0;32m----> 5\u001b[0m \u001b[43mskweak\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisplay_entities\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhmm\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/cca/lib/python3.9/site-packages/skweak/utils.py:746\u001b[0m, in \u001b[0;36mdisplay_entities\u001b[0;34m(doc, layer, add_tooltip)\u001b[0m\n\u001b[1;32m    744\u001b[0m         spans \u001b[38;5;241m=\u001b[39m get_spans(doc, matched_layers)\n\u001b[1;32m    745\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 746\u001b[0m         spans \u001b[38;5;241m=\u001b[39m \u001b[43mget_spans\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLayer type not accepted\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/cca/lib/python3.9/site-packages/skweak/utils.py:265\u001b[0m, in \u001b[0;36mget_spans\u001b[0;34m(doc, sources, labels)\u001b[0m\n\u001b[1;32m    263\u001b[0m                 spans\u001b[38;5;241m.\u001b[39mappend(span)\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 265\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnnotation source \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m cannot be found\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m source)\n\u001b[1;32m    267\u001b[0m \u001b[38;5;66;03m# Remove possible overlaps\u001b[39;00m\n\u001b[1;32m    268\u001b[0m spans \u001b[38;5;241m=\u001b[39m _remove_overlaps(spans)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Annotation source \"hmm\" cannot be found"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Ik heb een duo sim en een telefoon allebei van kpn en nu krijg ik weer geregeld voor werk en ons volgende week ook het geval was\")\n",
    "hmm.pipe(doc)\n",
    "skweak.utils.display_entities(doc, \"hmm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "44124869-56cb-4f0b-83ba-409991adde50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write to data/train_ner_3_7.spacy...done\n",
      "Write to data/test_ner_3_7.spacy...done\n"
     ]
    }
   ],
   "source": [
    "skweak.utils.docbin_writer(train, \"data/train_ner_3.spacy\")\n",
    "skweak.utils.docbin_writer(test, \"data/test_ner_3.spacy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "73fa972d-0b0f-4bf0-9220-d70660dcb06c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'DIEPROFUN': 5513, 'NETWORK': 1169, 'HARDWARE': 1342, 'TIME': 553, 'COMPETITOR': 0, 'PROBLEM': 3, 'PERSON': 21492, 'BILLING': 2, 'COMMUNICATION': 2, 'CONTRACT': 278, 'LOCATION': 0, 'MECHANIC': 1, 'PROCESS': 0, 'CONNECTION': 47, 'NUMBER': 74, 'ORDER': 0, 'OFFER': 0, 'PLANNING': 0, 'SEGMENT': 100, 'OTHER': 0}\n"
     ]
    }
   ],
   "source": [
    "labels_dict = dict.fromkeys(labels, 0)\n",
    "\n",
    "for doc in test:\n",
    "  for ent in doc.ents:\n",
    "    if ent.label_ in labels_dict:\n",
    "      labels_dict[ent.label_] +=1\n",
    "\n",
    "print(labels_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9317b058-6fc8-4082-a583-78bb7feda6e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 2100/2100 [00:24<00:00, 87.22it/s]\n"
     ]
    }
   ],
   "source": [
    "docs = [nlp(text) for text in tqdm(train)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4175751e-31b0-48c3-8723-17704f3102b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Spacy2Conll:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def convert(self, docs):\n",
    "        print(\"EXTENDED DOCS {}\".format(len(docs)))\n",
    "        self.result = list()\n",
    "        docstart = \"-DOCSTART-\"\n",
    "        empty = \"O\"\n",
    "        for index, doc in enumerate(tqdm(docs)):\n",
    "\n",
    "            self.result.append(docstart)\n",
    "            self.result.append(\"\\n\")\n",
    "            self.result.append(\"\\n\")\n",
    "            for t_index,tok in enumerate(doc):\n",
    "                    label = tok.ent_iob_\n",
    "                    if tok.ent_iob_ != \"O\":\n",
    "                        label += '-' + tok.ent_type_\n",
    "                    if not str(tok).isspace():\n",
    "\n",
    "                        self.result.append(str(t_index+1)  +\"\\t\" + str(tok) + \"\\t\" + label)\n",
    "                        self.result.append(\"\\n\")\n",
    "                    if str(tok) in [\".\", \"?\"]:\n",
    "                        self.result.append(\"\\n\")\n",
    "\n",
    "            self.result.append(\"\\n\")\n",
    "\n",
    "    def write(self, output_path: str):\n",
    "\n",
    "        with open(output_path, \"w\",encoding=\"utf-8\") as outfile:\n",
    "            outfile.write(\"\".join(self.result))\n",
    "        print(\"CoNLL done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "22f79042-d456-4afe-ba89-0086f9929c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = Spacy2Conll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cb9b9dd6-5a9a-4c76-a903-f0495d13d629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXTENDED DOCS 2044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 2044/2044 [00:00<00:00, 2621.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CoNLL done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "converter.convert(test)\n",
    "converter.write(\"data/test_4_14_.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "7b5c65fd-489c-431b-99fc-aec799ee80b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_list(sim_words, words):\n",
    "    \n",
    "    list_of_words = []\n",
    "    \n",
    "    for i in range(len(sim_words)):\n",
    "        \n",
    "        sim_words_list = list(sim_words[i])\n",
    "        sim_words_list.append(words)\n",
    "        sim_words_tuple = tuple(sim_words_list)\n",
    "        list_of_words.append(sim_words_tuple)\n",
    "        \n",
    "    return list_of_words\n",
    "\n",
    "input_word = 'buurt,draadloos,glasvezel,monteur,databundel'\n",
    "user_input = [x.strip() for x in input_word.split(',')]\n",
    "result_word = []\n",
    "    \n",
    "for words in user_input:\n",
    "    \n",
    "        sim_words = similarity_model.wv.most_similar(words, topn = 5)\n",
    "        sim_words = append_list(sim_words, words)\n",
    "            \n",
    "        result_word.extend(sim_words)\n",
    "    \n",
    "similar_word = [word[0] for word in result_word]\n",
    "similarity = [word[1] for word in result_word] \n",
    "similar_word.extend(user_input)\n",
    "labels = [word[2] for word in result_word]\n",
    "label_dict = dict([(y,x+1) for x,y in enumerate(set(labels))])\n",
    "color_map = [label_dict[x] for x in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "10512b1b-e593-497c-a12f-41090d461a88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"1020px\"\n",
       "    height=\"1020\"\n",
       "    src=\"iframe_figures/figure_121.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.io as pio\n",
    "pio.renderers.default = 'iframe'\n",
    "import numpy as np\n",
    "import plotly.graph_objs as go\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def display_pca_scatterplot_3D(model, user_input=None, words=None, label=None, color_map=None, topn=5, sample=2):\n",
    "\n",
    "    if words == None:\n",
    "        if sample > 0:\n",
    "            words = np.random.choice(list(model.vocab.keys()), sample)\n",
    "        else:\n",
    "            words = [ word for word in model.vocab ]\n",
    "    \n",
    "    word_vectors = np.array([model.wv[w] for w in words])\n",
    "    \n",
    "\n",
    "    two_dim = PCA(random_state=0).fit_transform(word_vectors)[:,:2]\n",
    "\n",
    "    data = []\n",
    "    count = 0\n",
    "    \n",
    "    for i in range (len(user_input)):\n",
    "\n",
    "                trace = go.Scatter(\n",
    "                    x = two_dim[count:count+topn,0], \n",
    "                    y = two_dim[count:count+topn,1],  \n",
    "                    # z = three_dim[count:count+topn,2],\n",
    "                    text = words[count:count+topn],\n",
    "                    name = user_input[i],\n",
    "                    textposition = \"top center\",\n",
    "                    textfont_size = 20,\n",
    "                    mode = 'markers+text',\n",
    "                    marker = {\n",
    "                        'size': 10,\n",
    "                        'opacity': 0.8,\n",
    "                        'color': 2\n",
    "                    }\n",
    "       \n",
    "                )\n",
    "                \n",
    "                # For 2D, instead of using go.Scatter3d, we need to use go.Scatter and delete the z variable. Also, instead of using\n",
    "                # variable three_dim, use the variable that we have declared earlier (e.g two_dim)\n",
    "            \n",
    "                data.append(trace)\n",
    "                count = count+topn\n",
    "\n",
    "    trace_input = go.Scatter(\n",
    "                    x = two_dim[count:,0], \n",
    "                    y = two_dim[count:,1],  \n",
    "                    # z = three_dim[count:,2],\n",
    "                    text = words[count:],\n",
    "                    name = 'input words',\n",
    "                    textposition = \"top center\",\n",
    "                    textfont_size = 20,\n",
    "                    mode = 'markers+text',\n",
    "                    marker = {\n",
    "                        'size': 10,\n",
    "                        'opacity': 1,\n",
    "                        'color': 'black'\n",
    "                    }\n",
    "                    )\n",
    "\n",
    "    # For 2D, instead of using go.Scatter3d, we need to use go.Scatter and delete the z variable.  Also, instead of using\n",
    "    # variable three_dim, use the variable that we have declared earlier (e.g two_dim)\n",
    "            \n",
    "    data.append(trace_input)\n",
    "    \n",
    "# Configure the layout\n",
    "\n",
    "    layout = go.Layout(\n",
    "        margin = {'l': 0, 'r': 0, 'b': 0, 't': 0},\n",
    "        showlegend=True,\n",
    "        legend=dict(\n",
    "        x=1,\n",
    "        y=0.5,\n",
    "        font=dict(\n",
    "            family=\"Courier New\",\n",
    "            size=25,\n",
    "            color=\"black\"\n",
    "        )),\n",
    "        font = dict(\n",
    "            family = \" Courier New \",\n",
    "            size = 15),\n",
    "        autosize = False,\n",
    "        width = 1000,\n",
    "        height = 1000\n",
    "        )\n",
    "\n",
    "\n",
    "    plot_figure = go.Figure(data = data, layout = layout)\n",
    "    plot_figure.show()\n",
    "\n",
    "    \n",
    "display_pca_scatterplot_3D(similarity_model, user_input, similar_word, labels, color_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a0715fb9-c067-4b37-a5ef-55c14aa761ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_training_data(spacy_docs: List, split: str, mode=\"BIO\"): #NEW VERSION WITH SENTENCES\n",
    "        \n",
    "    document_registry = list()\n",
    "    annotated_spacy_docs = spacy_docs\n",
    "    document = list()\n",
    "    \n",
    "    if mode == \"DOCRED\":\n",
    "        print(f'Output type: {mode}')\n",
    "    if mode == \"conllu\":\n",
    "        print(f'Output type: {mode}')\n",
    "        document.append(\"# global.columns = \"+(\"\\t\").join([\"id\",\"text\",\"ner\"]))\n",
    "        # document.append(\"\\n\")\n",
    "    for doc in annotated_spacy_docs:\n",
    "        \n",
    "        word_positions_dict = dict()\n",
    "        current_document = []\n",
    "    \n",
    "        tags=[token.ent_iob_+\"-\"+\"_\".join((token.ent_type_.split(\" \"))) if token.ent_iob_ != \"O\" else \"O\" for token in doc]\n",
    "        \n",
    "        biluo_tags = iob_to_biluo(tags)\n",
    "        \n",
    "        for i,tag in enumerate(biluo_tags):\n",
    "            if tag.startswith(\"U-\"):\n",
    "                biluo_tags[i] = tag.replace(\"U-\",\"S-\")\n",
    "            if tag.startswith(\"L-\"):\n",
    "                biluo_tags[i] = tag.replace(\"L-\",\"E-\")\n",
    "                \n",
    "        tags = biluo_tags\n",
    "                \n",
    "        document.append(\"-DOCSTART-\")\n",
    "        document.append(\"\\n\\n\")\n",
    "        \n",
    "\n",
    "        for i,word_tag in enumerate(zip(doc,tags)):\n",
    "            document.append((\"\\t\").join([str(word_tag[0]),str(word_tag[1])]))\n",
    "            if str(word_tag[0]) == \".\" or str(word_tag[0]) == \"?\":\n",
    "                document.append(\"\\n\")\n",
    "            document.append(\"\\n\")\n",
    "                \n",
    "        \n",
    "    output = f'data/randomSplits/{split}.txt'\n",
    "    with open(output, \"w\",encoding=\"utf-8\") as outfile:\n",
    "        outfile.write(\"\".join(document))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a20b14d8-03f5-4a76-b4a3-2763003f1852",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_training_data(train, \"train\",\"BIO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "19ac85b1-5484-4bb2-9091-67b7462b54e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1638/2067422047.py:4: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  selection = np.random.choice(train, size, replace=False)\n"
     ]
    }
   ],
   "source": [
    "sample_size = [100,200,500,1000,2000]\n",
    "\n",
    "for size in sample_size:\n",
    "    selection = np.random.choice(train, size, replace=False)\n",
    "    export_training_data(selection,f'train_{size}',\"BIO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e38c9e-e7e0-4e6c-af36-06d273bf48ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
